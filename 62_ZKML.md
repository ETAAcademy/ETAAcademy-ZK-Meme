# ETAAcademy-ZKMeme: 62. ZKML

<table>
  <tr>
    <th>title</th>
    <th>tags</th>
  </tr>
  <tr>
    <td>62. ZKML </td>
    <td>
      <table>
        <tr>
          <th>zk-meme</th>
          <th>basic</th>
          <th>quick_read</th>
          <td>ZKML</td>
        </tr>
      </table>
    </td>
  </tr>
</table>

[Github](https://github.com/ETAAcademy)｜[Twitter](https://twitter.com/ETAAcademy)｜[ETA-ZK-Meme](https://github.com/ETAAcademy/ETAAcademy-ZK-Meme)

Authors: [Evta](https://twitter.com/pwhattie), looking forward to your joining

# **Zero-Knowledge Machine Learning (ZKML): Verifiable AI without Sacrificing Privacy**

**Zero-Knowledge Machine Learning (ZKML)** is an emerging interdisciplinary field that combines zero-knowledge proof (ZKP) technology with machine learning (ML). The core idea is to encode the ML inference process into arithmetic circuits, enabling a prover to generate a cryptographic proof that the inference result was correctly computed from a given input and potentially private model parameters. A verifier can then validate this computation **without accessing the underlying input or model**, thereby addressing trust issues in Machine Learning as a Service (MLaaS).

ZKML faces several technical challenges, including the **high complexity of circuit construction**, the **difficulty of handling floating-point operations**, and the **conversion of non-linear functions** into arithmetic circuits. To overcome these obstacles, various optimization strategies are employed. These include decomposing numbers into digits, using comparators and truncation mechanisms, and converting non-linear functions like ReLU, Sigmoid, and Softmax into efficient small **lookup table (LUT)** representations. Techniques like **sparse ternary networks** are also leveraged to reduce the computational overhead.

In practical implementations, a typical ZKML system follows a **prover-verifier architecture**. The system takes an ONNX model exported from mainstream ML frameworks and parses it into a **directed acyclic graph (DAG)** representation. Each AI operation in the DAG is then mapped to a corresponding gate-level circuit. The full ZKML workflow includes **image preprocessing, model loading, proof generation**, and **on-chain or off-chain verification**.

---

**Zero-Knowledge Machine Learning (ZKML)** is an emerging interdisciplinary field that applies zero-knowledge proof (ZKP) techniques to machine learning (ML) tasks. The goal of ZKML is to ensure the **correctness and trustworthiness** of machine learning models during **training, inference, and testing**, all without revealing sensitive data or model details.

As **Machine Learning-as-a-Service (MLaaS)** becomes increasingly popular—where users send data to a third-party platform for inference tasks like disease prediction, image recognition, or financial risk assessment—serious privacy and trust issues arise. These ML services often operate as **black boxes**, providing results with no guarantee that the computations were performed honestly or according to specification.

#### Why ZKML Matters

Many users lack the computational resources (e.g., smartphones or IoT devices) to run large models locally, so they rely on **cloud-based services** for both training and inference. However, this places their **private data and models in potentially untrusted environments**.

ZKML offers a powerful cryptographic solution: it allows one party (the ML provider) to prove to another (the client) that a computation was performed correctly **without revealing the underlying data or model**. In the context of outsourced ML, this enables:

- **Privacy-preserving inference**
- **Auditable training**
- **Verifiable testing**

Despite promising applications, ZKML still faces several challenges, such as high circuit complexity, limited compiler performance, and poor compatibility with mainstream ML models. Ongoing research is addressing these bottlenecks through **circuit optimization**, **model-aware design**, and **efficient compilation strategies**.

#### What is a Zero-Knowledge Proof (ZKP)?

A **Zero-Knowledge Proof** is a cryptographic protocol that allows a prover to convince a verifier that a statement is true, without revealing anything beyond the truth of the statement itself.

In ML, ZKPs can be used to prove that:

- A model was trained using compliant data
- A prediction was indeed generated by a given model
- A test result is genuine and not forged

This leads to **ZKML**, where model behavior can be validated **without exposing** the model or the input data.

### A Simple ZKML Inference Flow

- The client sends an input `x` to the server.
- The server uses its private model `w` to compute the output `r = W(x, w)`.
- The input `x` and result `r` are **public inputs**, while the model `w` remains a **private witness**.
- The server generates a ZKP `π` to prove that `r` was correctly derived from `x` and `w`.

  - This is equivalent to proving that `F(x, r, w) = W(x, w) - r = 0`.

- The client verifies the proof `π`. If valid, the result `r` is accepted as trustworthy.

This enables **trustless yet verifiable ML services**.

#### A Primer on Machine Learning

Machine learning, a core area of artificial intelligence, enables systems to learn patterns from data and make predictions. ML can process various data types—numerical, textual, visual, or behavioral—and includes approaches like:

- **Supervised Learning** (most common in ZKML)
- **Unsupervised Learning**
- **Reinforcement Learning**

In supervised learning, the goal is to approximate a function $g(X)$ from labeled data $(X_i, Y_i)$ using a model $f_θ$, such that $f_θ(X) ≈ Y$.

- The training dataset: $D_{train} = {(X_1, Y_1), ..., (X_n, Y_n)}$
- Objective: Minimize a loss function $l(Y_i, f_θ(X_i))$ to find optimal parameters θ
- Once trained, the model can be used for predictions

Common models used in ZKML include:

- **Linear Regression (LR)**
- **Decision Trees (DT)**
- **Support Vector Machines (SVM)**
- **Multilayer Perceptrons (MLP)**
- **Convolutional Neural Networks (CNN)**

ZKML typically leverages **deep neural networks** due to their expressive power.

### Verifiable Machine Learning: What Needs to Be Proven?

As ML models become larger and training becomes more expensive, users increasingly **outsource computation** to service providers. This creates a critical question:

> How can users trust that the service provider actually performed the computation as claimed?

This is where **verifiable ML** comes in. It covers:

- **Verifiable Training**

  - **Private Witness**: Training data, labels
  - **Public Input**: Dataset hashes, loss thresholds
  - **Goal**: Prove that the model was trained correctly using the correct data and configuration
  - **Use Cases**: Outsourced or federated training

- **Verifiable Testing**

  - **Private Witness**: Model parameters
  - **Public Input**: Test data, labels, accuracy thresholds
  - **Goal**: Verify performance metrics are accurate
  - **Use Cases**: Auditing models before deployment

- **Verifiable Inference**

  - **Private Witness**: Model weights
  - **Public Input**: Inference input, output hash
  - **Goal**: Prove that the model output is correctly generated
  - **Use Cases**: On-chain AI, privacy-preserving APIs

| ZKML Task | Private Witness        | Public Input                      | Goal                     | Privacy Focus           | Application Examples               |
| --------- | ---------------------- | --------------------------------- | ------------------------ | ----------------------- | ---------------------------------- |
| Training  | Training data & labels | Hashes, loss thresholds           | Prove correct training   | Hide dataset & model    | Federated training, cloud services |
| Testing   | Model parameters       | Test data, labels, hash, accuracy | Validate performance     | Hide model architecture | Auditing, deployment assurance     |
| Inference | Model parameters       | Input data, output hash           | Prove correct prediction | Protect model privacy   | AI APIs, smart contracts           |

#### ZK Circuit Design in ZKML

At the core of a ZKP system is a **circuit**, which represents the logic to be proven. This circuit processes both public inputs and private witnesses and outputs a proof that the computation was done correctly. A typical ZK circuit for ML includes:

- **Circuit Input**:

  - Public: Input data, output result
  - Private: Model weights, internal activations

- **Circuit Logic**:

  - Matrix multiplications, activation functions, normalization, etc.

- **Circuit Output**:

  - Final prediction and the ZKP itself

These circuits are then fed into ZKP systems (e.g., Groth16, PLONK, Halo2), which generate succinct, verifiable proofs.

### Challenges and Solutions in Zero-Knowledge Machine Learning (ZKML)

Zero-Knowledge Machine Learning (ZKML) aims to enable verifiable inference of machine learning models without revealing sensitive information. However, building efficient and general-purpose ZKML systems remains a major challenge. These challenges fall into two primary categories:

#### Generality Limitations

Zero-knowledge proofs (ZKPs) operate over finite fields and only support linear arithmetic. As a result, they cannot natively handle two critical components of machine learning:

- **Floating-point computations**
- **Nonlinear functions** such as ReLU and Sigmoid

This mismatch means that machine learning models must be discretized or rewritten to fit within the ZKP framework. Such transformations can reduce both the generality and the performance of the original models.

#### Efficiency Barriers

Modern deep learning models are large and complex, with millions (or billions) of parameters. When these models are translated into arithmetic or Boolean circuits for ZKPs, the resulting proof size and computation time can become prohibitive.

For example, using Groth16 to prove inference for a VGG16 model requires over **14 trillion group exponentiations**, taking more than **30 years** to compute—even on a **256-core server**.

#### Addressing ZKML Challenges: Core Techniques

To overcome these barriers, recent work in ZKML has focused on several strategies. These fall under three main categories: quantization, nonlinear approximation, and protocol/circuit optimization.

Quantization converts floating-point values into integers, making them suitable for field arithmetic while preserving accuracy as much as possible.

| System         | Method                                                                 | Features                                          |
| -------------- | ---------------------------------------------------------------------- | ------------------------------------------------- |
| **SafetyNets** | Scales inputs with constants (e.g., ⌈βWᵢ⌉) to avoid overflows          | Ensures results remain within $-(p-1)/2, (p-1)/2$ |
| **VeriML**     | Multiplies inputs by $2^l$ to retain decimal precision                 | Preserves numeric accuracy                        |
| **zkCNN**      | Uses Jacob et al.'s affine quantization (a = L(q − Z))                 | Maps to Q-bit integers via affine encoding        |
| **Kang**       | Uses Halo2 + 8-bit weights and affine transform                        | Significantly reduces circuit size and proof time |
| **Mystique**   | Efficient conversions between float/fixed-point and arithmetic/Boolean | Developer-friendly via TensorFlow integration     |
| **ZEN**        | Sign-bit grouping + remainder-based checks                             | Shrinks convolution constraints                   |

Since ZKPs can't natively support nonlinearities, systems approximate them using low-degree polynomials, piecewise functions, or custom protocols.

| System         | Method                                                         | Features                                                  |
| -------------- | -------------------------------------------------------------- | --------------------------------------------------------- |
| **SafetyNets** | Simple quadratic function as ReLU proxy                        | Simple but low fidelity                                   |
| **VeriML**     | Remez algorithm for polynomial approximation of ReLU/Sigmoid   | Higher accuracy                                           |
| **zkMLaaS**    | Least-squares fitting to optimize function approximation       | Improves approximation quality                            |
| **zkCNN**      | Bit-decomposition to implement piecewise ReLU                  | High performance, higher complexity                       |
| **Fan**        | Uses Hadamard product on operator matrices                     | Accurate but adds computational cost                      |
| **zkDL**       | zkReLU protocol with helper inputs + GKR + inner product proof | Supports both forward & backward passes, highly efficient |

Reducing the cost of ZKML involves optimizing circuits, operators, and protocols:

- **Efficient circuit structures** (e.g., modular layers, matrix representations)
- **Operator optimizations** (e.g., lookup tables for nonlinearities)
- **Protocol-level improvements** (e.g., GKR, Probabilistic Proofs)

| System              | Optimization Approach                              | Effect                                             |
| ------------------- | -------------------------------------------------- | -------------------------------------------------- |
| **SafetyNets**      | GKR-based matrix multiplication verification       | Good inference efficiency; ReLU still a bottleneck |
| **Hydra**           | Transformer-friendly SNARK circuit design          | Efficient for large transformer models             |
| **pvCNN**, **vCNN** | Convolution & nonlinear layer circuit optimization | Significant reduction in proof/verification time   |
| **Kang**            | Halo2 with lookup tables and custom gates          | Over 10× speedup (e.g., for MobileNet v2)          |

### **ZKML in Practice: From Research Frontier to Real-World Applications**

Zero-Knowledge Machine Learning (ZKML) is no longer confined to academic exploration. It’s rapidly entering the commercial landscape, powering new kinds of secure, private, and verifiable AI applications across diverse domains—from DeFi to CAPTCHA verification, on-chain agents to fully trustless AI deployments.

Below are some of the most notable projects and platforms demonstrating how ZKML is making an impact today:

#### **EZKL**: From PyTorch to zk-SNARK in Seconds

**EZKL** is a lightweight Rust-based toolkit and library that bridges mainstream ML frameworks with ZK circuits. It enables developers to convert deep learning models (defined in PyTorch or TensorFlow and exported to ONNX) into zk-SNARK circuits, allowing for verifiable inference with minimal overhead.

- **Workflow**: PyTorch/TF model → ONNX → input JSON → EZKL conversion → ZK circuit + proof generation.
- **Performance**: For an MNIST model, proof generation takes \~2 seconds with \~700MB RAM usage.
- **Adoption**: Frequently used in hackathons, EZKL is beginner-friendly and provides an end-to-end ZKML toolchain for engineers.

#### **Mina Protocol (formerly Coda)**: Recursive zk-SNARKs for Scalable Blockchains

Mina is a Layer-1 blockchain protocol whose defining feature is the use of recursive zk-SNARKs to compress and validate the blockchain state:

- **State Size**: The entire blockchain remains \~20KB, regardless of historical data.
- **Efficiency**: Verifiers don’t need to sync the full chain, only validate a single proof.
- **Significance to ZKML**: While not directly ZKML, Mina’s recursive proof system sets the stage for recursive neural network verification—an important future direction for complex model deployments.

#### **DeFiChain**: ZK-Proofed Fraud Detection and Credit Scoring

DeFiChain integrates ZKML to validate AI-based fraud detection and credit scoring models without compromising user data privacy.

- **Use Case**: Verifying model inference (e.g., logistic regression) via zk-SNARKs.
- **Privacy**: Protects both user inputs and model parameters.
- **Compliance**: An example of ZKML’s potential in regulatory-friendly and privacy-preserving financial AI.

#### **Modulus Labs**: On-Chain AI Trading Bots and Game Engines

Modulus Labs is pushing the boundaries of ZKML commercialization with fully on-chain AI systems:

- **RockyBot**: The world’s first AI trading bot fully deployed on-chain. Uses LSTM and RNNs for prediction; zk-SNARKs prove every trade follows the model’s decision logic.
- **Leela vs. the World**: A chess engine where users play against a Leela-like AI, with ZK proofs validating that the neural network made the move.
- **Research**: The team has also benchmarked ZKML performance across various model sizes and systems.

#### **Giza**: Trustless AI Deployment on StarkNet

**Giza** is a protocol focused on fully trust-minimized AI model deployment, tightly integrated with the StarkNet ecosystem.

- **Workflow**:

  - Model Format: ONNX
  - **Giza Transpiler**: Converts ONNX models into **Cairo**, the language of StarkNet
  - **ONNX-Cairo Runtime**: Executes AI models within the StarkNet VM
  - **Giza Model Contracts**: Smart contracts to deploy and invoke models on-chain

- **Vision**: Seamless ZKML integration with ZK-rollup-friendly blockchains for fully verifiable AI on-chain.

#### **ZKaptcha**: ZKML for Smarter CAPTCHA and Human Verification

**ZKaptcha** addresses the problem of bot abuse in Web3 platforms by bringing intelligence and privacy to CAPTCHA systems:

- **Mechanism**: User completes a CAPTCHA → generates a ZK proof → smart contract verifies the proof.
- **ZKML Integration**: Enhances CAPTCHA with AI-based behavioral analysis—such as mouse movement patterns or human activity signals.
- **Potential**: Foundational for Anti-Sybil defenses, ZK Proof-of-Personhood, and privacy-preserving human verification in Web3.

#### **ZKPyTorch**: Lowering the Barrier for AI Engineers

While most current ZKML systems require deep knowledge of cryptography, **ZKPyTorch** makes ZKML more accessible by automatically compiling PyTorch models into proof-generating circuits.

- **Key Components**:

  1. **ZKP Preprocessor**: Converts PyTorch models into DAG-structured computation graphs with intermediate representations, witness hints, and constraint metadata for ZK systems (e.g., Expander).
  2. **ZK-Friendly Quantization Module**: Optimizes bit-width (e.g., int8, int4) for small finite fields like $M_{61} = 2^{61} - 1$, reducing circuit complexity and increasing proving efficiency.
  3. **Hierarchical Circuit Optimizer**:

     - _Model-level_: Fuses subgraphs, token batching for LLM inference.
     - _Operation-level_: Replaces costly operations (e.g., softmax) with efficient ZK alternatives (e.g., lookup tables).
     - _Circuit-level_: Enables multicore parallelism, constraint folding, and recursion support.

- **Outlook**: Future expansions may support ResNet, ViT, Transformers, and even ZK proofs for model _training_, not just inference.

#### Summary Table

| Project/Company   | Type          | Core Application                | Highlights                                                               |
| ----------------- | ------------- | ------------------------------- | ------------------------------------------------------------------------ |
| **EZKL**          | Toolchain     | Model conversion + inference    | Fast integration, hackathon-ready                                        |
| **Mina Protocol** | L1 Blockchain | State compression + sync        | Recursive ZK for infinite scalability                                    |
| **DeFiChain**     | DeFi Platform | Fraud detection, credit scoring | Privacy-preserving AI for regulatory contexts                            |
| **Modulus Labs**  | Startup       | Trading bots, chess engine      | RNNs + ZK for real-time on-chain AI                                      |
| **Giza**          | ZKML Platform | Trustless AI on StarkNet        | Cairo + ONNX integration, full on-chain inference                        |
| **ZKaptcha**      | Bot Defense   | CAPTCHA + Human Verification    | ZKML-powered anti-Sybil, behavioral pattern analysis                     |
| **ZKPyTorch**     | Compiler Tool | Model-to-ZKP compiler           | PyTorch native, quantization, IR conversion, hierarchical circuit design |

### Unlocking the Future of ZKML

While ZKML (Zero-Knowledge Machine Learning) has already produced promising prototypes and early applications, there remain significant challenges before it can achieve widespread adoption. Several key development directions are emerging to address these hurdles:

#### Improving Computational Efficiency and Scalability

Current ZKML systems are mostly limited to small-scale models such as MNIST classifiers. Scaling to deep neural networks (DNNs), multimodal models (e.g., image + text), or generative models like GANs and Transformers is still far from practical. Proof generation and verification remain computationally intensive, demanding high CPU/GPU resources and significant memory.

**Future directions include:**

- **Optimizing recursive proof systems**, such as Plonky2 and Nova, to enable fast and scalable aggregation of proofs across multiple layers or inputs.
- **Designing ZK-friendly circuits** tailored for deep learning, with efficient implementations of operations like sparse matrix multiplication and non-linear activation functions.
- **Developing domain-specific ZK protocols**, purpose-built for neural network architectures such as CNNs or Transformers, to reduce overhead and improve performance.

#### Enhancing Usability and Practicality

Many ZKML frameworks today are still research-oriented and require deep cryptographic knowledge to use. The lack of mature tooling and standard APIs makes it difficult for enterprises or developers from traditional AI backgrounds to adopt these solutions.

**Key development goals:**

- **Building developer-friendly tools and platforms**, such as IDE plugins, automated circuit generators, and prebuilt proof modules for common models.
- **Creating industry-specific solutions**, tailored to sectors with strong privacy and compliance needs (e.g., finance, healthcare, Web3, supply chain).
- **Seamless integration with mainstream ML frameworks**, allowing models built in PyTorch or TensorFlow to automatically compile into ZK-proof-enabled formats with minimal friction.

#### Supporting Complex and Diverse Model Architectures

Most current ZKML implementations support only simple feedforward neural networks or even just linear models. More advanced architectures—such as GPT models, diffusion-based generators (e.g., Stable Diffusion), or self-supervised learning systems—are far more challenging to support due to their size and computational complexity. Moreover, unsupervised and iterative optimization processes are difficult to represent as circuits.

**Future opportunities include:**

- **Designing ZK-friendly protocols for generative models**, such as verifying each step in a diffusion model with high efficiency.
- **Enabling multimodal inference**, with joint circuit representations for models that take both visual and textual input.
- **Verifying training and optimization processes**, including SGD, Adam, and other optimizers, by building ZK circuits for each iteration or gradient update.

#### Strengthening Privacy and Security Guarantees

ZKML today primarily focuses on privacy during the inference phase, ensuring that inputs or model parameters aren’t exposed. However, the training phase can also leak sensitive data (as models may memorize or reflect private user information). Furthermore, existing ZKML systems offer limited protection against adversarial attacks such as malicious inputs or adversarial perturbations.

**Key development directions:**

- **Verifiable training processes**, ensuring that models were trained honestly and according to specified rules, without compromising the privacy of the underlying data.
- **Combining ZKML with other privacy-preserving technologies**, such as fully homomorphic encryption (FHE) or secure multi-party computation (MPC), to enable end-to-end privacy from training to inference.
- **Adversarial input detection**, using zero-knowledge proofs to verify the validity of inputs and defend against manipulated or out-of-distribution samples.

---

## 1. ZKML Optimization: Transforming Nonlinear Functions

One of the most promising directions in ZKML (Zero-Knowledge Machine Learning) optimization involves transforming nonlinear functions into more ZKP-friendly representations. A notable example is the use of **sparse ternary networks**—models whose weights are constrained to the set {−1, 0, 1}. These networks retain strong performance through careful training and offer three key benefits:

1. **Sparsity** (many weights are zero), allowing computation to be skipped.
2. **Simplified arithmetic**, where multiplications reduce to additions or subtractions.
3. **Integer-based computation** (e.g., INT8 instead of FP32), making it more ZK-efficient.

Such networks can be paired with **sparsity-aware GKR protocols**, working in conjunction with specialized sparse Sumcheck variants (like Lasso or SpaSum), and can support arbitrary neural network architectures.

This section focuses on **transforming nonlinear functions**—a common bottleneck in ZKML—into efficient forms for zero-knowledge proof systems.

### Lookup-Based Representations of Nonlinear Functions

Nonlinear functions (e.g., ReLU, GELU, Softmax) are notoriously expensive to prove in ZK. Traditional methods rely on:

- **Bit decomposition**, which introduces significant constraint overhead, or
- **Lookup tables**, which can become too large and inefficient.

A more scalable alternative is to **approximate or encode nonlinear functions via compact table lookups**. The idea is to predefine a public table of valid input-output pairs for a function (e.g., `y = f(x)`), and require the prover to prove that their `(x, y)` pair exists in this table.

To make this process efficient, ZK frameworks can use several **small and reusable building blocks**:

- **Digital Decomposition**: Breaks input values into smaller chunks (e.g., bitwise or digit-wise), facilitating efficient lookups and reducing table size.
- **Comparison Protocols**: Enables operations like `max(0, x)` (used in ReLU) through table-based comparisons.
- **Truncation Modules**: Helps approximate float/decimal values into lower-precision representations while controlling errors and simplifying circuits.

Using these components, provable versions of nonlinear operations like **ReLU**, **Sigmoid**, and **Normalization** can be efficiently implemented in ZKML.

#### 1. Digital Decomposition

To avoid massive lookup tables, an input `x` is split into several segments:

$x = x_{k-1} ∥ x_{k-2} ∥ ... ∥ x_0,\quad x_i ∈ \{0,1\}^{d_i}$

Since ZK circuits operate over finite fields (Fp), attackers can exploit modular equivalence (`x` vs. `x + p`). To prevent this, a **positive-value decomposition protocol** is used, ensuring `x < p` via additional comparison checks.

#### 2. Comparison Protocol

The goal is to prove that a secret input `x` is less than a public constant `c` (`x < c`). A recursive breakdown is employed:

$1\{x < c\} = 1\{x_1 < c_1\} + 1\{x_1 = c_1\} \cdot 1\{x_0 < c_0\}$

This reduces to 2k lookups and k multiplications for a k-segment value. To optimize, the results of $z_{lt}$ (less-than) and $z_{eq}$ (equality) can be merged into a single lookup, reducing the overall table count from 2k to k+1.

#### 3. Truncation

While truncation for positive values is straightforward (just drop low-order bits), **negative numbers require special handling**. In Fp, negative values are represented as `p − |x|`. Simple truncation will fail in this case.

Instead, truncation mimics **2’s complement right shifts**:

- Convert x to its 2's complement $\bar{x}$ (bitwise inversion).
- Perform positive-value truncation on $\bar{x}$ to get $\bar{y}$.
- Convert $\bar{y}$ back to negative form to get the final truncated result `y`.

This process can be proven sound using a combination of comparison and decomposition protocols.

#### 4. Most Significant Nonzero Bit (MSNZB)

To find the position of the first nonzero bit in `x` (from most significant to least), satisfying:

$2^y ≤ x ≤ 2^{y+1} - 1$

Standard MPC approaches decompose `x` into multiple chunks and repeatedly apply MSNZB logic, which is computationally expensive.

Instead, in ZK, the prover proposes a candidate range $[2^y, 2^{y+1} - 1]$, and the verifier simply checks if `x` falls within this range. This reduces the proof to a single lookup table of size `log(p)`, drastically lowering complexity.

### ZK-Friendly Implementations of Common Nonlinear Functions in Neural Networks

In zero-knowledge machine learning (ZKML), proving the correctness of nonlinear functions efficiently is one of the major challenges. Below, we outline how several commonly used nonlinear functions—such as ReLU, MaxPooling, Sigmoid, Softmax, GELU, and Normalization—can be transformed into ZK-friendly circuits using a combination of optimized protocols such as comparison, exponentiation, division, and check-zero. These implementations focus on minimizing arithmetic constraints and making efficient use of lookup tables.

#### ReLU: Rectified Linear Unit

The ReLU function is defined as:

$y := x \cdot \mathbf{1}\{x \geq 0\}$

In ZK, this is implemented through a **comparison protocol** that checks whether $x \geq 0$. The result acts as a binary selector:

- If true, $y = x$
- If false, $y = 0$

This is effectively **conditional multiplication** inside a zero-knowledge circuit. The prover securely demonstrates that the chosen output corresponds to the correct branch without revealing the actual value of $x$.

#### MaxPooling

Given a set of inputs $x_0, x_1, \ldots, x_{n-1}$, the MaxPooling function computes:

$y := \max(x_0, x_1, ..., x_{n-1})$

To prove this in ZK:

- Use **comparison protocols** to verify that $y \geq x_i$ for all $i$, confirming $y$ is at least as large as every input.
- Use the **CheckZero protocol** to ensure that $y$ equals one of the inputs:

$\prod_{i=0}^{n-1} (y - x_i) = 0$

This guarantees that the claimed maximum isn't fabricated outside the input set.

#### Sigmoid Function

The Sigmoid activation is defined as:

$y := \frac{1}{1 + e^{-x}}$

It is split into two branches depending on the sign of $x$:

- If $x \geq 0$, compute $y = \frac{1}{1 + e^{-x}}$
- If $x < 0$, use the equivalent $y = \frac{e^{-x}}{1 + e^{-x}}$

**ZK implementation**:

- A **comparison protocol** to determine the sign of $x$
- An **exponentiation protocol** to evaluate $e^{-x}$
- A **division protocol** to compute the final result

This piecewise definition reduces complexity and ensures precision control in field arithmetic.

#### Softmax Function

For each element $y_i$ in a vector, Softmax is defined as:

$y_i := \frac{e^{x_i - x_{\text{max}}}}{\sum_{j=0}^{n-1} e^{x_j - x_{\text{max}}}}$

Key steps in ZK implementation:

- Use **MaxPooling** to find $x_{\text{max}}$, improving numerical stability
- Use **exponentiation protocols** for $e^{x_i - x_{\text{max}}}$
- Use a **division protocol** for normalization across the sum

This normalization not only improves numerical stability but also reduces bit-length requirements for field representation—e.g., for a 256-class output, only \~20 bits are needed.

#### GELU: Gaussian Error Linear Unit

GELU is an advanced activation function:

$y := 0.5 \cdot x \cdot \left(1 + \tanh\left(\sqrt{2/\pi} \cdot (x + 0.044715x^3)\right)\right)$

Given that:

$\tanh(x) = 2 \cdot \text{Sigmoid}(2x) - 1$

**ZK implementation**:

- Reusing the **Sigmoid protocol**
- Implementing polynomial evaluation (up to cubic terms)
- Performing scalar multiplications

These computations can be efficiently modeled using low-degree polynomials and basic arithmetic circuits.

#### Normalization (e.g., BatchNorm, LayerNorm)

Normalization applies the transformation:

$y_i := \gamma \cdot \frac{x_i - \mu}{\sqrt{\sigma}} + \beta$

Where:

- $\mu$ is the mean of the inputs
- $\sigma$ is the variance

**ZK implementation**:

- Use a **sum protocol** to compute the mean and variance
- Use a **square root inverse protocol** to normalize
- Apply scalar multiplication for affine transformation with $\gamma$ and $\beta$

This decomposition ensures efficiency and composability in larger ZKML circuits.

---

## 2. ONNX Models, Provers, and Verifiers in zkML

Zero-Knowledge Machine Learning (zkML) enables the transformation of machine learning models (such as ONNX models) into verifiable circuits compatible with zero-knowledge proof (ZKP) systems. This allows users to **load and run pre-trained models (e.g., MNIST digit recognition)**, **generate ZK proofs for inference results**, and **publicly verify correctness and privacy**—all without revealing the private input or model details.

### From AI Models to Zero-Knowledge Proofs

The Open Neural Network Exchange (ONNX) format serves as a universal model representation across AI frameworks. zkML systems convert ONNX models into ZK-verifiable circuits, enabling compatibility with major deep learning platforms like PyTorch, TensorFlow, and Scikit-learn.

The **Prover** is responsible for parsing the ONNX model, executing the model's inference on private inputs using a ZKP-friendly representation, and generating a zero-knowledge proof (e.g., a zk-SNARK). This proof certifies that the inference was carried out correctly using the given model and input. The **Verifier**, which can be deployed on-chain, validates this proof without access to private data or internal model parameters—ensuring both correctness and privacy.

#### zkML Architecture Overview

```text
[ONNX Model] + [Private Input]
       │
       ▼
     [Prover]
  └──→ Runs forward inference
  └──→ Generates ZK proof
       │
       ▼
     [CLI Tool] → Deploys verifier to blockchain
       │
       ▼
 [Verifier (on-chain)]
  └──→ Verifies proof validity
```

zkML represents neural networks as **Directed Acyclic Graphs (DAGs)**, a standard in many AI compilers such as TensorFlow XLA, TVM, and ONNX Runtime. Within this representation, nodes can be of two types:

- `Node`: Standard operations like MatMul, ReLU, etc.
- `SubGraph`: Control-flow logic such as loops or conditionals, recursively represented as sub-models.

zkML uses `tract-onnx` to parse ONNX models into internal representations called `ParsedNodes`. During this process, various model attributes—like `kernel_shape`, `strides`, and `padding` for convolution layers—are interpreted and mapped to ZK circuit components. Inputs and outputs are tagged as either private or public and each ONNX operator is converted into a corresponding ZK circuit substructure. This makes it possible to **train models using PyTorch/TensorFlow → export to ONNX → convert into ZK circuits with zkML**.

<details><summary><b> Code </b></summary>

```rust
#[derive(Clone, Debug, Serialize, Deserialize, Default)]
pub struct Model {
    pub graph: ParsedNodes,            // Each node in the graph represents an operation (e.g., MatMul, ReLU)
    pub visibility: VarVisibility,     // Indicates whether each variable is private or public
}

#[derive(Clone, Debug, Serialize, Deserialize)]
pub enum NodeType {
    Node(SerializableNode),
    SubGraph {
        model: Box,
        inputs: Vec,
        idx: usize,
        out_dims: Vec<Vec>,
        out_scales: Vec,
        output_mappings: Vec<Vec>,
        input_mappings: Vec,
    },
}

fn topological_sort(&self) -> Result<Vec, GraphError> {
    let mut visited = HashMap::new();
    let mut sorted = Vec::new();
    // Logic to visit nodes in dependency order...
}

pub fn load_onnx_model(
    path: &str,
    run_args: &RunArgs,
    visibility: &VarVisibility,
) -> Result<ParsedNodes, GraphError> {
    let (model, symbol_values) = Self::load_onnx_using_tract(path, run_args)?;
    // Further parsing and validation...
}
```

</details>

#### Witness Construction and Circuit Generation

The ZK prover executes **forward inference** across all graph nodes and collects the intermediate values (witnesses) representing each computational step. These are embedded into the circuit. A PLONK-style polynomial commitment scheme is used to generate the proof from these witnesses.

The **verifier** only checks the correctness of the mapping from `public_inputs → public_outputs`. The `private_inputs` remain confidential, but correctness of computation is still provable.

To support deep learning computations in ZK, AI operations must be **circuitized**, meaning they are expressed at the level of basic arithmetic gates. This translation process is central to zkML: it involves converting floating-point math, nonlinear functions, and complex layers into efficient ZK circuit representations. The system dynamically computes circuit size based on model complexity, which affects domain size (for polynomial interpolation) and the number of additional rows needed for zero-knowledge (zk-rows).

<details><summary><b> Code </b></summary>

```rust
pub fn prove(&self, inputs: &[Vec]) -> Result<ProverOutput, String> {
    let (witness, outputs) = self.create_witness(inputs)?;
    let proof = ProverProof::create(
        &group_map,
        witness,
        &[],
        &self.prover_index,
        &mut rng,
    )?;
    Ok(ProverOutput { output: Some(outputs), proof, .. })
}
```

</details>

#### Verifying on the Blockchain

Each model corresponds to a unique circuit and thus requires a **separately deployed verifier smart contract**. zkML provides a CLI tool that auto-generates the verifier contract (using `o1js`), allowing users to deploy it on-chain, submit proofs, upload public inputs, and check proof validity.

A REST API or zkML verifier tool can serve as a unified verification entry point, supporting multiple users and syncing with blockchain state.

<details><summary><b> Code </b></summary>

```rust
pub fn verify(
    &self,
    proof: &ProverProof<Vesta, ZkOpeningProof>,
    public_inputs: Option<&[Vec]>,
    public_outputs: Option<&[Vec]>,
) -> Result<bool, String> {
    let result = kimchi::verifier::verify(
        &group_map,
        &self.verifier_index,
        proof,
        &public_values,
    );
    result.map(|_| true)
}

OnnxOperation::MatMul { m, n, k } => {
    for _i in 0..*m {
        for _j in 0..*n {
            for _l in 0..*k {
                gates.push(CircuitGate::new(
                    GateType::ForeignFieldMul,
                    [Wire::new(current_row, 0); 7],
                    vec![],
                ));
                current_row += 1;
            }
        }
    }
    Ok(gates)
}

fn calculate_domain_params(circuit_size: usize) -> (usize, usize) {
    let lookup_domain_size = 0;
    let circuit_lower_bound = std::cmp::max(circuit_size, lookup_domain_size + 1);
    // Logic to calculate domain size and zk_rows...
}

```

</details>

#### Gate-Level Circuit Mapping Example

Matrix multiplication in ONNX (`MatMul`) is mapped to foreign field multiplication gates in the ZK circuit. Each iteration creates gate entries for multiplying input matrix elements. And domain parameters are computed based on the total circuit size and lookup table requirements.

<details><summary><b> Code </b></summary>

```rust
OnnxOperation::MatMul { m, n, k } => {
    for _i in 0..*m {
        for _j in 0..*n {
            for _l in 0..*k {
                gates.push(CircuitGate::new(
                    GateType::ForeignFieldMul,
                    [Wire::new(current_row, 0); 7],
                    vec![],
                ));
                current_row += 1;
            }
        }
    }
    Ok(gates)
}
```

```rust
fn calculate_domain_params(circuit_size: usize) -> (usize, usize) {
    let lookup_domain_size = 0;
    let circuit_lower_bound = std::cmp::max(circuit_size, lookup_domain_size + 1);
    // Compute next power of two, add zk_rows, etc.
}
```

</details>

### Zero-Knowledge Machine Learning (zkML) Example: Inference with Privacy and Proofs

Zero-Knowledge Machine Learning (zkML) combines machine learning inference with zero-knowledge proofs, allowing one to prove that a model was correctly executed on a given input—without revealing sensitive data or internal model parameters. This article walks through a basic zkML pipeline: preprocessing data, loading a model, generating proofs, verifying inference results, and even deploying proof verification on-chain.

### Image Preprocessing and Model Initialization in zkML

Before running a machine learning model in a zero-knowledge (zk) setting, proper **image preprocessing** is essential. For example, if the model expects a **28×28 grayscale image**—as is the case with the MNIST digit classification dataset—the image must be resized and converted to the appropriate format. Furthermore, each pixel value needs to be **normalized** to match the distribution the model was trained on. A common normalization formula for MNIST is:

$\text{normalized\_pixel} = \frac{x}{255.0} - 0.1307 \Big/ 0.3081$

This centers the pixel values around the dataset's mean (0.1307) and scales them according to its standard deviation (0.3081), improving inference accuracy.

Once the input is preprocessed, the **model must be loaded** and properly configured. This involves specifying runtime parameters using `RunArgs`. For instance, you might set `batch_size = 1` to process one image at a time.

Visibility settings are also crucial in zkML. The `VarVisibility` configuration determines whether the **inputs and outputs** of the model will be **public or private**—an important distinction in privacy-preserving machine learning. For example, both input and output can be marked as public for demo purposes, but in privacy-sensitive applications, one or both would remain private.

The model is loaded using the `Model::new(...)` function, which takes in the ONNX model file, the runtime arguments, and visibility settings. Internally, the model is parsed into a graph structure (`ParsedNodes`) that can be transformed into a zero-knowledge circuit for proving and verification.

This setup phase lays the foundation for secure, privacy-preserving inference, where both the computation and the result can be proven correct—without revealing sensitive input data.

<details><summary><b> Code </b></summary>

```rust
fn preprocess_image(img_path: &str) -> Result<Vec, Box> {
    let img = image::open(img_path)?.into_luma8();
    let resized = image::imageops::resize(&img, 28, 28, image::imageops::FilterType::Lanczos3);
    let pixels: Vec = resized.into_raw().into_iter().map(|x| x as f32).collect();
    let pixels = pixels.into_iter()
        .map(|x| (x / 255.0 - 0.1307) / 0.3081) // Normalize with mean and standard deviation
        .collect();
    Ok(pixels)
}
```

</details>

#### Creating the zkML Prover and Verifier Systems

A fundamental part of zero-knowledge machine learning (zkML) is the creation of a **prover** and a **verifier** system. These two components work together to ensure that model inference was performed correctly and honestly—without requiring anyone to see the actual input data.

The `ProverSystem` takes a pre-trained machine learning model and converts its computation graph into a corresponding **zero-knowledge circuit**. It then builds a **witness** (the intermediate computation data) and generates a **zero-knowledge proof** that the inference was correctly executed on a given input.

The `VerifierSystem` is responsible for validating this proof. It checks whether the provided `proof`, `input`, and `output` are consistent with one another—essentially verifying that the model inference actually occurred as claimed. This mechanism ensures that the model’s result is genuine and not fabricated.

During inference, the system processes an image input and simultaneously generates a proof. The raw model outputs, usually logits, are converted into class probabilities using the **softmax** function. The system prints out the prediction alongside its confidence levels.

To demonstrate the security of the system, we can simulate an attack scenario: for example, by altering the expected output and attempting to pass off a forged prediction. If the verifier correctly detects the inconsistency and marks the proof as **invalid**, it confirms the **robustness** of the zkML framework. An invalid result indicates that the system can successfully prevent tampering and manipulation.

Beyond local verification, zkML also supports **on-chain verification**. Each model can be associated with its own dedicated **zkML verifier smart contract** on a blockchain. Because each machine learning model results in a unique circuit, it requires a tailored verifier contract. Proofs and inputs can be submitted to these contracts using REST APIs or CLI tools. This integration allows blockchain systems to validate off-chain ML computations in a **trustless and transparent** manner.

With zkML, we enable a new paradigm: **verifiable machine learning**, where anyone can confirm that a model was run correctly—without ever seeing the input or compromising privacy.

<details><summary><b> Code </b></summary>

```rust
fn main() -> Result<(), Box<dyn std::error::Error>> {
// 1. Setup model
let mut variables = HashMap::new();
variables.insert("batch_size".to_string(), 1);
let run_args = RunArgs { variables };

    let visibility = VarVisibility {
        input: Visibility::Public,
        output: Visibility::Public,
    };

    let model = Model::new("models/kmeans.onnx", &run_args, &visibility)?;

    // 2. Create prover system
    let prover = ProverSystem::new(&model);
    let verifier = prover.verifier();

    // Load first image
    let input1 = preprocess_image("models/data/1052.png")?;
    let input_vec1 = vec![input1];

    // Generate output and proof for first image
    let prover_output1 = prover.prove(&input_vec1)?;
    let output1 = prover_output1.output.as_ref().expect("Output should be public");

    // Verify proof for first image
    let is_valid1 = verifier.verify(&prover_output1.proof, Some(&input_vec1), Some(output1))?;
    println!(
        "Verification result: {}",
        if is_valid1 { "✓ Valid" } else { "✗ Invalid" }
    );

}

fn print_prediction_info(logits: &[f32]) {
    let max_logit = logits.iter().take(10).fold(f32::NEG_INFINITY, |a, &b| a.max(b));
    let exp_sum: f32 = logits.iter().take(10).map(|&x| (x - max_logit).exp()).sum();
    let softmax: Vec = logits.iter().take(10).map(|&x| ((x - max_logit).exp()) / exp_sum).collect();

    println!("Probabilities for each digit:");
    for (digit, prob) in softmax.iter().enumerate() {
        println!("Digit {}: {:.4}", digit, prob);
    }
    println!("Predicted digit: {}", softmax.iter().enumerate().max_by(|(_, a), (_, b)| a.partial_cmp(b).unwrap()).map(|(digit, _)| digit).unwrap());

}
```

</details>

#### Summary of zkML Capabilities

| Feature                 | Description                                                         |
| ----------------------- | ------------------------------------------------------------------- |
| **Model Loading**       | Parses ONNX models into internal computation graphs                 |
| **Input Preprocessing** | Normalizes and reshapes data (e.g., images) for model compatibility |
| **ZKP Generation**      | Produces zk-SNARK proofs of correct inference                       |
| **Proof Verification**  | Public verification of model outputs without leaking sensitive data |
| **On-Chain Support**    | Deploys zk-verifier contracts for blockchain integration            |
